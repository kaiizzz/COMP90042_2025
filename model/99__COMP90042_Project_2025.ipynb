{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# **2025 COMP90042 Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# **Readme**\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "- All original data files, including the `evidence.json` file should be saved into `./data`. Processed data will be saved to `./data/preprocessed`. Not required to create a new directory for processed files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# **1. DataSet Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "path = \"data/\"\n",
        "# Load JSON files\n",
        "with open(path+\"train-claims.json\", \"r\") as f:\n",
        "    train_claims = json.load(f)\n",
        "\n",
        "with open(path+\"dev-claims.json\", \"r\") as f:\n",
        "    dev_claims = json.load(f)\n",
        "\n",
        "with open(path+\"dev-claims-baseline.json\", \"r\") as f:\n",
        "    dev_baseline_claims = json.load(f)\n",
        "\n",
        "with open(path+\"test-claims-unlabelled.json\", \"r\") as f:\n",
        "    test_claims = json.load(f)\n",
        "\n",
        "with open(path+\"evidence.json\", \"r\") as f:\n",
        "    evidence = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrames\n",
        "train_df = pd.DataFrame(train_claims)\n",
        "dev_df = pd.DataFrame(dev_claims)\n",
        "dev_baseline_df = pd.DataFrame(dev_baseline_claims)\n",
        "test_df = pd.DataFrame(test_claims)\n",
        "evidence_df = pd.DataFrame(list(evidence.items()), columns=[\"key\", \"value\"])\n",
        "\n",
        "train_df = pd.DataFrame(train_claims).transpose()\n",
        "dev_df = pd.DataFrame(dev_claims).transpose()\n",
        "dev_baseline_df = pd.DataFrame(dev_baseline_claims).transpose()\n",
        "test_df = pd.DataFrame(test_claims).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Step 1: Text Normalisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text normalization - lowercase and make alphanumeric\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Step 2. Stopword Removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove stop words\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Step 3. Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = text.split()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Step 4. Tokenisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    return text.split(\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Apply Preprocessing Steps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/5] train-claims        : ████████████████ Done!\n",
            "[2/5] dev-claims          : ████████████████ Done!\n",
            "[3/5] dev-claims-baseline : ████████████████ Done!\n",
            "[4/5] test-claims         : ████████████████ Done!\n",
            "[5/5] evidence            : ████████████████ Done!\n"
          ]
        }
      ],
      "source": [
        "datasets = [\n",
        "    (train_df, (f\"{\"train-claims\":<20}\", \"claim_text\")), \n",
        "    (dev_df, (f\"{\"dev-claims\":<20}\", \"claim_text\")), \n",
        "    (dev_baseline_df, (f\"{\"dev-claims-baseline\":<20}\", \"claim_text\")), \n",
        "    (test_df, (f\"{\"test-claims\":<20}\", \"claim_text\")),\n",
        "    (evidence_df, (f\"{\"evidence\":<20}\", \"value\"))\n",
        "    ]\n",
        "steps = [normalize_text, remove_stopwords, lemmatize_text, tokenize_text]\n",
        "progress_bar = len(steps)*4\n",
        "\n",
        "# Processing steps for all datasets\n",
        "ds = 1\n",
        "for df, (dataset, col) in datasets:\n",
        "    print(f\"[{ds}/{len(datasets)}] {dataset}: {progress_bar * \"▒\"}\", end=\"\\r\")\n",
        "    i = 0\n",
        "    for step in steps:\n",
        "        i += 4\n",
        "        df[col] = df[col].apply(step)\n",
        "        print(f\"[{ds}/{len(datasets)}] {dataset}: {i * \"█\"}{(progress_bar-i)*\"▒\"}\", end=\"\\r\")\n",
        "    print(f\"[{ds}/{len(datasets)}] {dataset}: {progress_bar * \"█\"} Done!\")\n",
        "    ds += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Saving Processed Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created directory ./data/preprocessed/\n",
            "[FINISHED] Saved preprocessed data to JSON files...\n"
          ]
        }
      ],
      "source": [
        "# Save processed data\n",
        "path = \"data/preprocessed/\"\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    print(\"Created directory ./data/preprocessed/\")\n",
        "\n",
        "train_df.to_json(path+\"preprocessed_train.json\", orient=\"index\")\n",
        "dev_df.to_json(path+\"preprocessed_dev.json\", orient=\"index\")\n",
        "dev_baseline_df.to_json(path+\"preprocessed_dev_baseline.json\", orient=\"index\")\n",
        "test_df.to_json(path+\"preprocessed_test.json\", orient=\"index\")\n",
        "evidence_df.to_json(path+\"preprocessed_evidence.json\", orient=\"records\")\n",
        "\n",
        "print(f\"[FINISHED] Saved preprocessed data to JSON files...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
