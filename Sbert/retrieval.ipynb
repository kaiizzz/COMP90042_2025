{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dc9eda",
   "metadata": {},
   "source": [
    "## Sentence Bert Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edd0981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at C:\\Users\\BILLZH~1\\AppData\\Local\\Temp\\matplotlib-_rb12bol because the default path (C:\\Users\\Bill Zhu\\.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bill Zhu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.losses import CachedMultipleNegativesRankingLoss \n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced03efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path = '../data/'\n",
    "\n",
    "with open(path+\"train-claims.json\", \"r\") as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_claims).transpose()\n",
    "\n",
    "with open(path+\"dev-claims.json\", \"r\") as f:\n",
    "    dev_claims = json.load(f)\n",
    "dev_df = pd.DataFrame(dev_claims).transpose()\n",
    "\n",
    "with open(path+\"evidence.json\", \"r\") as f:\n",
    "    evidence = json.load(f)\n",
    "evidence_df = pd.DataFrame(list(evidence.items()), columns=[\"key\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b7ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['claim', 'evidence', 'label'],\n",
      "    num_rows: 16402\n",
      "})\n",
      "{'claim': 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'evidence': 'Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "claims = []\n",
    "evidence_texts = []\n",
    "labels = []\n",
    "\n",
    "evidence_map = dict(evidence)\n",
    "evidence_ids = list(evidence_map.keys())\n",
    "\n",
    "for _, info in train_claims.items():\n",
    "    claim_text = info[\"claim_text\"]\n",
    "    positive_ids = set(info[\"evidences\"])  # ensure no duplicates\n",
    "\n",
    "    # === Add all golden (positive) evidences ===\n",
    "    for eid in positive_ids:\n",
    "        if eid in evidence_map:\n",
    "            claims.append(claim_text)\n",
    "            evidence_texts.append(evidence_map[eid])\n",
    "            labels.append(1)\n",
    "\n",
    "    # === Add at least 5 unique negatives ===\n",
    "    negatives_added = 0\n",
    "    tried_ids = set()\n",
    "\n",
    "    while negatives_added < 10:\n",
    "        neg_id = random.choice(evidence_ids)\n",
    "\n",
    "        if neg_id in positive_ids or neg_id in tried_ids:\n",
    "            continue\n",
    "\n",
    "        tried_ids.add(neg_id)\n",
    "\n",
    "        if neg_id in evidence_map:\n",
    "            claims.append(claim_text)\n",
    "            evidence_texts.append(evidence_map[neg_id])\n",
    "            labels.append(-1)\n",
    "            negatives_added += 1\n",
    "\n",
    "# onvert to HuggingFace Dataset\n",
    "data = {\n",
    "    \"claim\": claims,\n",
    "    \"evidence\": evidence_texts,\n",
    "    \"label\": labels\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Confirm\n",
    "print(dataset)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3bfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "model2 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model3 = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "loss1 = MultipleNegativesRankingLoss(model1)\n",
    "loss2 = MultipleNegativesRankingLoss(model2)\n",
    "loss3 = MultipleNegativesRankingLoss(model3)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab598393",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"output\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"sentence-transformer-training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bf362b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65637a243924baaa78a2cb994b49e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='2769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  96/2769 00:04 < 02:18, 19.36 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.328800</td>\n",
       "      <td>3.189373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m trainer1 \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[0;32m      8\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel1,\n\u001b[0;32m      9\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss1,\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m trainer1\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_models/model_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:2627\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2626\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2627\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:3103\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3100\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 3103\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:3200\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial)\u001b[0m\n\u001b[0;32m   3198\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3199\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[1;32m-> 3200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy\u001b[38;5;241m.\u001b[39mSTEPS, SaveStrategy\u001b[38;5;241m.\u001b[39mEPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_global_step:\n\u001b[0;32m   3203\u001b[0m     best_checkpoint_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\trainer.py:3902\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   3899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 3902\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   3905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\trainer.py:917\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m    914\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    915\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model checkpoint to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 917\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;66;03m# Transformers v4.46.0 changed the `tokenizer` attribute to a more general `processing_class` attribute\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse_version(transformers_version) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m parse_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.46.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\SentenceTransformer.py:1297\u001b[0m, in \u001b[0;36mSentenceTransformer.save_pretrained\u001b[1;34m(self, path, model_name, create_model_card, train_datasets, safe_serialization)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_pretrained\u001b[39m(\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1279\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1283\u001b[0m     safe_serialization: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;124;03m    Saves a model and its configuration files to a directory, so that it can be loaded\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;124;03m    with ``SentenceTransformer(path)`` again.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;124;03m            the traditional (but unsafe) PyTorch way.\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_model_card\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_model_card\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\SentenceTransformer.py:1239\u001b[0m, in \u001b[0;36mSentenceTransformer.save\u001b[1;34m(self, path, model_name, create_model_card, train_datasets, safe_serialization)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;66;03m# Try to save with safetensors, but fall back to the traditional PyTorch way if the module doesn't support it\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1239\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1241\u001b[0m     module\u001b[38;5;241m.\u001b[39msave(model_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\models\\Transformer.py:514\u001b[0m, in \u001b[0;36mTransformer.save\u001b[1;34m(self, output_path, safe_serialization)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_path: \u001b[38;5;28mstr\u001b[39m, safe_serialization: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_path)\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_bert_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fOut:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_utils.py:3564\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   3559\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m   3561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[0;32m   3562\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[0;32m   3563\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[1;32m-> 3564\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3566\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "\n",
    "trainer1 = SentenceTransformerTrainer(\n",
    "        model=model1,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        loss=loss1,\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer1.train()\n",
    "\n",
    "# Save the model\n",
    "trainer1.save_model(f\"out_models/model_0\")\n",
    "\n",
    "trainer2 = SentenceTransformerTrainer(\n",
    "       model=model2,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        loss=loss2,\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer2.train()\n",
    "\n",
    "# Save the model\n",
    "trainer2.save_model(f\"out_models/model_1\")\n",
    "\n",
    "trainer3 = SentenceTransformerTrainer(\n",
    "        model=model3,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        loss=loss3,\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer3.train()\n",
    "\n",
    "# Save the model\n",
    "trainer3.save_model(f\"out_models/model_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539672f",
   "metadata": {},
   "source": [
    "### First Stage Revrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5cf67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('out_models/model_0')\n",
    "model2 = SentenceTransformer('out_models/model_1')\n",
    "model3 = SentenceTransformer('out_models/model_2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e1064df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e9ca7fd9554279b4e3b34fb8f2b914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3429c2eb2d64a6495340f4d4fbb6b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781d9d8ef5fa45c1b4a44720f33c3420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings1 = model1.encode(evidence_df['value'].tolist(), show_progress_bar=True, device='cuda')\n",
    "embeddings2 = model2.encode(evidence_df['value'].tolist(), show_progress_bar=True, device='cuda')\n",
    "embeddings3 = model3.encode(evidence_df['value'].tolist(), show_progress_bar=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e678ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bill Zhu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\SentenceTransformer.py:649: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfe3828a21d4ffda1e19b38a8dc5635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d8008f36084fbcb82674d90bffc475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42ec8e7743e49c3988b862eda216d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "claim_embeddings1 = model1.encode(train_df['claim_text'], show_progress_bar=True, device='cuda')\n",
    "claim_embeddings2 = model2.encode(train_df['claim_text'], show_progress_bar=True, device='cuda')\n",
    "claim_embeddings3 = model3.encode(train_df['claim_text'], show_progress_bar=True, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43cf0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "total_embeddings = (embeddings1 + embeddings2 + embeddings3) / 3\n",
    "claim_embeddings = (claim_embeddings1 + claim_embeddings2 + claim_embeddings3) / 3\n",
    "\n",
    "similarity_matrix = cosine_similarity(claim_embeddings, total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b1ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "top_k = 25\n",
    "top_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]  # sort descending, get top 100 indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ad598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"top_100_evidence\"] = [\n",
    "    evidence_df.iloc[indices][\"key\"].tolist() for indices in top_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8830739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0679\n",
      "Recall: 0.5056\n",
      "F1-score: 0.1197\n"
     ]
    }
   ],
   "source": [
    "total_matches = 0\n",
    "total_gold = 0\n",
    "total_predicted = 0\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    gold_evidence = train_df[\"evidences\"].iloc[i]\n",
    "    predicted_evidence = train_df[\"top_100_evidence\"].iloc[i]\n",
    "\n",
    "    matches = sum(1 for ev in gold_evidence if ev in predicted_evidence)\n",
    "\n",
    "    total_matches += matches\n",
    "    total_gold += len(gold_evidence)\n",
    "    total_predicted += len(predicted_evidence)  # Should be 100 for each if consistent\n",
    "\n",
    "# Compute precision, recall, and F1-score\n",
    "precision = total_matches / total_predicted if total_predicted > 0 else 0\n",
    "recall = total_matches / total_gold if total_gold > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709e9b2",
   "metadata": {},
   "source": [
    "### Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692a311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['claim', 'evidence', 'label'],\n",
      "    num_rows: 32738\n",
      "})\n",
      "{'claim': ['Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'], 'evidence': ['Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.', 'At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.', 'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.', 'Recent publications include one on the importance of bees to the ecosystem overall.', 'Experimental biospheres, also called closed ecological systems, have been created to study ecosystems and the potential for supporting life outside the earth.'], 'label': [1, 1, 1, -1, -1]}\n"
     ]
    }
   ],
   "source": [
    "## setup dataset.\n",
    "claims = []\n",
    "evidence_texts = []\n",
    "labels = []\n",
    "\n",
    "evidence_map = dict(evidence)\n",
    "evidence_ids = list(evidence_map.keys())\n",
    "\n",
    "for row in train_df.itertuples():\n",
    "    claim_text = row.claim_text\n",
    "    positive_ids = set(row.evidences)\n",
    "    negative_ids = set(row.top_100_evidence) - positive_ids\n",
    "\n",
    "    # === Add all golden (positive) evidences ===\n",
    "    for eid in positive_ids:\n",
    "        if eid in evidence_map:\n",
    "            claims.append(claim_text)\n",
    "            evidence_texts.append(evidence_map[eid])\n",
    "            labels.append(1)\n",
    "    \n",
    "    for eid in negative_ids:\n",
    "        if eid in evidence_map:\n",
    "            claims.append(claim_text)\n",
    "            evidence_texts.append(evidence_map[eid])\n",
    "            labels.append(-1)\n",
    "\n",
    "# onvert to HuggingFace Dataset\n",
    "data = {\n",
    "    \"claim\": claims,\n",
    "    \"evidence\": evidence_texts,\n",
    "    \"label\": labels\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Confirm\n",
    "print(dataset)\n",
    "print(dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 08:39:45 - Use pytorch device: cuda:0\n",
      "2025-05-16 08:40:18 - NanoBEIR Evaluation of the model on ['msmarco', 'nfcorpus', 'nq'] dataset:\n",
      "2025-05-16 08:40:18 - Evaluating NanoMSMARCO_R100\n",
      "2025-05-16 08:40:18 - CrossEncoderRerankingEvaluator: Evaluating the model on the NanoMSMARCO_R100 dataset:\n",
      "2025-05-16 08:40:35 - Queries: 50\tPositives: Min 1.0, Mean 1.0, Max 1.0\tNegatives: Min 99.0, Mean 99.0, Max 99.0\n",
      "2025-05-16 08:40:35 -          Base  -> Reranked\n",
      "2025-05-16 08:40:35 - MAP:     48.96 -> 9.21\n",
      "2025-05-16 08:40:35 - MRR@10:  47.75 -> 6.16\n",
      "2025-05-16 08:40:35 - NDCG@10: 54.04 -> 10.77\n",
      "2025-05-16 08:40:35 - \n",
      "2025-05-16 08:40:35 - Evaluating NanoNFCorpus_R100\n",
      "2025-05-16 08:40:35 - CrossEncoderRerankingEvaluator: Evaluating the model on the NanoNFCorpus_R100 dataset:\n",
      "2025-05-16 08:42:04 - Queries: 50\tPositives: Min 1.0, Mean 50.4, Max 463.0\tNegatives: Min 54.0, Mean 92.8, Max 100.0\n",
      "2025-05-16 08:42:04 -          Base  -> Reranked\n",
      "2025-05-16 08:42:04 - MAP:     26.10 -> 28.06\n",
      "2025-05-16 08:42:04 - MRR@10:  49.98 -> 37.07\n",
      "2025-05-16 08:42:04 - NDCG@10: 32.50 -> 24.08\n",
      "2025-05-16 08:42:04 - \n",
      "2025-05-16 08:42:04 - Evaluating NanoNQ_R100\n",
      "2025-05-16 08:42:04 - CrossEncoderRerankingEvaluator: Evaluating the model on the NanoNQ_R100 dataset:\n",
      "2025-05-16 08:42:49 - Queries: 50\tPositives: Min 1.0, Mean 1.1, Max 2.0\tNegatives: Min 98.0, Mean 99.0, Max 100.0\n",
      "2025-05-16 08:42:49 -          Base  -> Reranked\n",
      "2025-05-16 08:42:49 - MAP:     41.96 -> 14.39\n",
      "2025-05-16 08:42:49 - MRR@10:  42.67 -> 12.65\n",
      "2025-05-16 08:42:49 - NDCG@10: 50.06 -> 16.83\n",
      "2025-05-16 08:42:49 - \n",
      "2025-05-16 08:42:49 - CrossEncoderNanoBEIREvaluator: Aggregated Results:\n",
      "2025-05-16 08:42:49 -          Base  -> Reranked\n",
      "2025-05-16 08:42:49 - MAP:     39.01 -> 17.22\n",
      "2025-05-16 08:42:49 - MRR@10:  46.80 -> 18.63\n",
      "2025-05-16 08:42:49 - NDCG@10: 45.54 -> 17.23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/461 05:46 < 14:38:43, 0.01 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import traceback\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers.cross_encoder import (\n",
    "    CrossEncoder,\n",
    "    CrossEncoderModelCardData,\n",
    "    CrossEncoderTrainer,\n",
    "    CrossEncoderTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.cross_encoder.evaluation import CrossEncoderNanoBEIREvaluator\n",
    "from sentence_transformers.cross_encoder.losses import CachedMultipleNegativesRankingLoss\n",
    "from sentence_transformers.cross_encoder.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n",
    "train_batch_size = 64\n",
    "num_epochs = 1\n",
    "num_rand_negatives = 5  # How many random negatives should be used for each question-answer pair\n",
    "\n",
    "model = CrossEncoder(\"cross-encoder/stsb-roberta-base\")\n",
    "\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42069)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]\n",
    "\n",
    "# evaluator = CrossEncoderNanoBEIREvaluator(\n",
    "#     dataset_names=[\"msmarco\", \"nfcorpus\", \"nq\"],\n",
    "#     batch_size=train_batch_size,\n",
    "# )\n",
    "#evaluator(model)\n",
    "\n",
    "# Set up training arguments\n",
    "args = CrossEncoderTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=f\"out_models/cross_encoder\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    run_name=\"run_cross\",  # Will be used in W&B if `wandb` is installed\n",
    "    seed=12,\n",
    ")\n",
    "\n",
    "trainer = CrossEncoderTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    loss=loss,\n",
    "    #evaluator=evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#evaluator(model)\n",
    "\n",
    "final_output_dir = f\"models/run_cross/final\"\n",
    "model.save_pretrained(final_output_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
