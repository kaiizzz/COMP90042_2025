{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dc9eda",
   "metadata": {},
   "source": [
    "## Sentence Bert Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0edd0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch \n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.losses import CoSENTLoss\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ced03efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path = '../data/'\n",
    "\n",
    "with open(path+\"train-claims.json\", \"r\") as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_claims).transpose()\n",
    "\n",
    "with open(path+\"dev-claims.json\", \"r\") as f:\n",
    "    dev_claims = json.load(f)\n",
    "dev_df = pd.DataFrame(dev_claims).transpose()\n",
    "\n",
    "with open(path+\"evidence.json\", \"r\") as f:\n",
    "    evidence = json.load(f)\n",
    "evidence_df = pd.DataFrame(list(evidence.items()), columns=[\"key\", \"value\"])\n",
    "\n",
    "with open(path+\"hard_negatives.json\", \"r\") as f:\n",
    "    hard_negatives = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01b7ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "positives = []\n",
    "hard_negs = []\n",
    "\n",
    "evidence_map = dict(evidence)\n",
    "evidence_ids = list(evidence_map.keys())\n",
    "\n",
    "# For each claim, add positives and hard negatives\n",
    "for id, info in train_claims.items():\n",
    "    # Anchor, positive, negative triplets\n",
    "    for pos_id in info[\"evidences\"]:\n",
    "        for neg_id in hard_negatives[id]:\n",
    "            claims.append(info[\"claim_text\"])\n",
    "            positives.append(evidence_map[pos_id])\n",
    "            hard_negs.append(evidence_map[neg_id])\n",
    "\n",
    "    # Hard negatives\n",
    "    # for neg_id in hard_negatives[id]:\n",
    "    #     claims.append(info[\"claim_text\"])\n",
    "    #     evidence_texts.append(evidence_map[neg_id])\n",
    "    #     labels.append(0)\n",
    "    \n",
    "\n",
    "# for _, info in train_claims.items():\n",
    "#     claim_text = info[\"claim_text\"]\n",
    "#     positive_ids = set(info[\"evidences\"])  # ensure no duplicates\n",
    "\n",
    "#     # === Add all golden (positive) evidences ===\n",
    "#     for eid in positive_ids:\n",
    "#         if eid in evidence_map:\n",
    "#             claims.append(claim_text)\n",
    "#             evidence_texts.append(evidence_map[eid])\n",
    "#             labels.append(1)\n",
    "\n",
    "#     # === Add at least 5 unique negatives ===\n",
    "#     negatives_added = 0\n",
    "#     tried_ids = set()\n",
    "\n",
    "#     while negatives_added < 10:\n",
    "#         neg_id = random.choice(evidence_ids)\n",
    "\n",
    "#         if neg_id in positive_ids or neg_id in tried_ids:\n",
    "#             continue\n",
    "\n",
    "#         tried_ids.add(neg_id)\n",
    "\n",
    "#         if neg_id in evidence_map:\n",
    "#             claims.append(claim_text)\n",
    "#             evidence_texts.append(evidence_map[neg_id])\n",
    "#             labels.append(-1)\n",
    "#             negatives_added += 1\n",
    "\n",
    "# convert to HuggingFace Dataset\n",
    "data = {\n",
    "    \"claim\": claims,\n",
    "    \"positive\": positives,\n",
    "    \"hard_negative\": hard_negs\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Confirm\n",
    "# print(dataset)\n",
    "# print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e3bfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "model2 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model3 = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "loss1 = MultipleNegativesRankingLoss(model1)\n",
    "loss2 = MultipleNegativesRankingLoss(model2)\n",
    "loss3 = MultipleNegativesRankingLoss(model3)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab598393",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"output\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=400,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=400,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=400,\n",
    "    run_name=\"sentence-transformer-training\",\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97fea67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9867' max='9867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9867/9867 09:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.051240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.054207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.057476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.056499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.051830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.051336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.046891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.047405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.047101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.044710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.041071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.042596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.040082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.039983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.037886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.038560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.037944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.038880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.036955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.036170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.036027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.035262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.034916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model 1\n",
    "model1.to(device)\n",
    "trainer1 = SentenceTransformerTrainer(\n",
    "        model=model1,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        loss=loss1,\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer1.train()\n",
    "\n",
    "# Save the model\n",
    "trainer1.save_model(f\"out_models/model_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "713f5ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb562f7e7b74c5daf4d05cc4f7805b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9867' max='9867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9867/9867 09:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.385400</td>\n",
       "      <td>1.073813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.940700</td>\n",
       "      <td>0.709425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.665200</td>\n",
       "      <td>0.465106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.475700</td>\n",
       "      <td>0.331138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>0.250588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.188991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.144086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.118779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.152700</td>\n",
       "      <td>0.109786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.098437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.081652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.075159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.070919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.064930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.061669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.057827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.055941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.054295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.053827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.050817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.049906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.049074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.047714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.047138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model 2\n",
    "model2.to(device)\n",
    "trainer2 = SentenceTransformerTrainer(\n",
    "       model=model2,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        loss=loss2,\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer2.train()\n",
    "\n",
    "# Save the model\n",
    "trainer2.save_model(f\"out_models/model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2922e0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d89c070554e17b12843b6bf643b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9867' max='9867' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9867/9867 15:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.278500</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.816200</td>\n",
       "      <td>0.595223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>0.376929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.259661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.187110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.137826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.109140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.143900</td>\n",
       "      <td>0.099014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.085716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.075084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.067323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.062108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.059784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.056255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.052150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.048476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.048302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.045886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.044986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.043348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.042116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.041314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.040195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.039608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training model 3\n",
    "model3.to(device)\n",
    "trainer3 = SentenceTransformerTrainer(\n",
    "        model=model3,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        loss=loss3,\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer3.train()\n",
    "\n",
    "# Save the model\n",
    "trainer3.save_model(f\"out_models/model_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539672f",
   "metadata": {},
   "source": [
    "### First Stage Revrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5cf67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('out_models/model_0')\n",
    "model2 = SentenceTransformer('out_models/model_1')\n",
    "model3 = SentenceTransformer('out_models/model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1064df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359118f4ff0b453181b55a24a0dc88c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d128ad83204209a9c11f7657c8e8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings1 = model1.encode(evidence_df['value'].tolist(), show_progress_bar=True, device='cuda')\n",
    "embeddings2 = model2.encode(evidence_df['value'].tolist(), show_progress_bar=True, device='cuda')\n",
    "embeddings3 = model3.encode(evidence_df['value'].tolist(), show_progress_bar=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e678ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasindu\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:649: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efb15ca9ec746a4aec71432ec24a6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900e37c03f904a0d812c48ee40f93bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "claim_embeddings1 = model1.encode(train_df['claim_text'], show_progress_bar=True, device='cuda')\n",
    "claim_embeddings2 = model2.encode(train_df['claim_text'], show_progress_bar=True, device='cuda')\n",
    "claim_embeddings3 = model3.encode(train_df['claim_text'], show_progress_bar=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43cf0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "total_embeddings = (embeddings1 + embeddings2 + embeddings3) / 3\n",
    "claim_embeddings = (claim_embeddings1 + claim_embeddings2 + claim_embeddings3) / 3\n",
    "\n",
    "similarity_matrix = cosine_similarity(claim_embeddings, total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd804c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeddings = embeddings1\n",
    "claim_embeddings = claim_embeddings1\n",
    "\n",
    "similarity_matrix = cosine_similarity(claim_embeddings, total_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34b1ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "top_k = 100\n",
    "top_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_k]  # sort descending, get top 100 indices\n",
    "\n",
    "train_df[\"top_100_evidence\"] = [\n",
    "    evidence_df.iloc[indices][\"key\"].tolist() for indices in top_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8830739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 performance, training set:\n",
      "Total matches: 4106/4122\n",
      "Precision: 0.9961\n",
      "Recall: 0.0334\n",
      "F1-score: 0.0647\n"
     ]
    }
   ],
   "source": [
    "total_matches = 0\n",
    "total_gold = 0\n",
    "total_predicted = 0\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    gold_evidence = train_df[\"evidences\"].iloc[i]\n",
    "    predicted_evidence = train_df[\"top_100_evidence\"].iloc[i]\n",
    "\n",
    "    matches = sum(1 for ev in gold_evidence if ev in predicted_evidence)\n",
    "\n",
    "    total_matches += matches\n",
    "    total_gold += len(gold_evidence)\n",
    "    total_predicted += len(predicted_evidence)  # Should be 100 for each if consistent\n",
    "\n",
    "# Compute precision, recall, and F1-score\n",
    "precision = total_matches / total_gold if total_gold > 0 else 0\n",
    "recall = total_matches / total_predicted if total_predicted > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"model1 performance, training set:\")\n",
    "print(f\"Total matches: {total_matches}/{total_gold}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99bc94e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3803\n"
     ]
    }
   ],
   "source": [
    "print(total_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709e9b2",
   "metadata": {},
   "source": [
    "### Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['claim', 'evidence', 'label'],\n",
      "    num_rows: 32738\n",
      "})\n",
      "{'claim': ['Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'], 'evidence': ['Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.', 'At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.', 'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.', 'Recent publications include one on the importance of bees to the ecosystem overall.', 'Experimental biospheres, also called closed ecological systems, have been created to study ecosystems and the potential for supporting life outside the earth.'], 'label': [1, 1, 1, -1, -1]}\n"
     ]
    }
   ],
   "source": [
    "## setup dataset.\n",
    "claims = []\n",
    "evidence_texts = []\n",
    "labels = []\n",
    "\n",
    "evidence_map = dict(evidence)\n",
    "evidence_ids = list(evidence_map.keys())\n",
    "\n",
    "for row in train_df.itertuples():\n",
    "    claim_text = row.claim_text\n",
    "    positive_ids = set(row.evidences)\n",
    "    negative_ids = set(row.top_100_evidence) - positive_ids\n",
    "\n",
    "    # === Add all golden (positive) evidences ===\n",
    "    for pos_id in positive_ids:\n",
    "        if pos_id in evidence_map:\n",
    "            claims.append(claim_text)\n",
    "            evidence_texts.append(evidence_map[pos_id])\n",
    "            labels.append(1)\n",
    "    \n",
    "    for pos_id in negative_ids:\n",
    "        if pos_id in evidence_map:\n",
    "            claims.append(claim_text)\n",
    "            evidence_texts.append(evidence_map[pos_id])\n",
    "            labels.append(-1)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "data = {\n",
    "    \"claim\": claims,\n",
    "    \"evidence\": evidence_texts,\n",
    "    \"label\": labels\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Confirm\n",
    "print(dataset)\n",
    "print(dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 08:39:45 - Use pytorch device: cuda:0\n",
      "2025-05-16 08:40:18 - NanoBEIR Evaluation of the model on ['msmarco', 'nfcorpus', 'nq'] dataset:\n",
      "2025-05-16 08:40:18 - Evaluating NanoMSMARCO_R100\n",
      "2025-05-16 08:40:18 - CrossEncoderRerankingEvaluator: Evaluating the model on the NanoMSMARCO_R100 dataset:\n",
      "2025-05-16 08:40:35 - Queries: 50\tPositives: Min 1.0, Mean 1.0, Max 1.0\tNegatives: Min 99.0, Mean 99.0, Max 99.0\n",
      "2025-05-16 08:40:35 -          Base  -> Reranked\n",
      "2025-05-16 08:40:35 - MAP:     48.96 -> 9.21\n",
      "2025-05-16 08:40:35 - MRR@10:  47.75 -> 6.16\n",
      "2025-05-16 08:40:35 - NDCG@10: 54.04 -> 10.77\n",
      "2025-05-16 08:40:35 - \n",
      "2025-05-16 08:40:35 - Evaluating NanoNFCorpus_R100\n",
      "2025-05-16 08:40:35 - CrossEncoderRerankingEvaluator: Evaluating the model on the NanoNFCorpus_R100 dataset:\n",
      "2025-05-16 08:42:04 - Queries: 50\tPositives: Min 1.0, Mean 50.4, Max 463.0\tNegatives: Min 54.0, Mean 92.8, Max 100.0\n",
      "2025-05-16 08:42:04 -          Base  -> Reranked\n",
      "2025-05-16 08:42:04 - MAP:     26.10 -> 28.06\n",
      "2025-05-16 08:42:04 - MRR@10:  49.98 -> 37.07\n",
      "2025-05-16 08:42:04 - NDCG@10: 32.50 -> 24.08\n",
      "2025-05-16 08:42:04 - \n",
      "2025-05-16 08:42:04 - Evaluating NanoNQ_R100\n",
      "2025-05-16 08:42:04 - CrossEncoderRerankingEvaluator: Evaluating the model on the NanoNQ_R100 dataset:\n",
      "2025-05-16 08:42:49 - Queries: 50\tPositives: Min 1.0, Mean 1.1, Max 2.0\tNegatives: Min 98.0, Mean 99.0, Max 100.0\n",
      "2025-05-16 08:42:49 -          Base  -> Reranked\n",
      "2025-05-16 08:42:49 - MAP:     41.96 -> 14.39\n",
      "2025-05-16 08:42:49 - MRR@10:  42.67 -> 12.65\n",
      "2025-05-16 08:42:49 - NDCG@10: 50.06 -> 16.83\n",
      "2025-05-16 08:42:49 - \n",
      "2025-05-16 08:42:49 - CrossEncoderNanoBEIREvaluator: Aggregated Results:\n",
      "2025-05-16 08:42:49 -          Base  -> Reranked\n",
      "2025-05-16 08:42:49 - MAP:     39.01 -> 17.22\n",
      "2025-05-16 08:42:49 - MRR@10:  46.80 -> 18.63\n",
      "2025-05-16 08:42:49 - NDCG@10: 45.54 -> 17.23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/461 05:46 < 14:38:43, 0.01 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import traceback\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers.cross_encoder import (\n",
    "    CrossEncoder,\n",
    "    CrossEncoderModelCardData,\n",
    "    CrossEncoderTrainer,\n",
    "    CrossEncoderTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.cross_encoder.evaluation import CrossEncoderNanoBEIREvaluator\n",
    "from sentence_transformers.cross_encoder.losses import CachedMultipleNegativesRankingLoss\n",
    "from sentence_transformers.cross_encoder.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n",
    "train_batch_size = 64\n",
    "num_epochs = 1\n",
    "num_rand_negatives = 5  # How many random negatives should be used for each question-answer pair\n",
    "\n",
    "model = CrossEncoder(\"cross-encoder/stsb-roberta-base\")\n",
    "\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42069)\n",
    "train_ds = split_dataset[\"train\"]\n",
    "eval_ds = split_dataset[\"test\"]\n",
    "\n",
    "# evaluator = CrossEncoderNanoBEIREvaluator(\n",
    "#     dataset_names=[\"msmarco\", \"nfcorpus\", \"nq\"],\n",
    "#     batch_size=train_batch_size,\n",
    "# )\n",
    "#evaluator(model)\n",
    "\n",
    "# Set up training arguments\n",
    "args = CrossEncoderTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=f\"out_models/cross_encoder\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    run_name=\"run_cross\",  # Will be used in W&B if `wandb` is installed\n",
    "    seed=12,\n",
    ")\n",
    "\n",
    "trainer = CrossEncoderTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    loss=loss,\n",
    "    #evaluator=evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#evaluator(model)\n",
    "\n",
    "final_output_dir = f\"models/run_cross/final\"\n",
    "model.save_pretrained(final_output_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
