{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2025 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qvff21Hv8zjk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1208827 evidence documents.\n",
      "Loaded 1228 train items.\n",
      "Loaded 154 dev items.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "data_dir = \"data\"\n",
    "train_claims_file = os.path.join(data_dir, \"train-claims.json\")\n",
    "dev_claims_file = os.path.join(data_dir, \"dev-claims.json\")\n",
    "test_claims_file = os.path.join(data_dir, \"test_claims_retrieved_reranked.json\")\n",
    "evidence_file = os.path.join(data_dir, \"evidence.json\")\n",
    "\n",
    "sep_token = \"[SEP]\"\n",
    "cls_token = \"[CLS]\"\n",
    "\n",
    "id2labels = [\"SUPPORTS\", \"NOT_ENOUGH_INFO\", \"REFUTES\", \"DISPUTED\"]\n",
    "labels2id = {\"SUPPORTS\": 0, \"NOT_ENOUGH_INFO\": 1, \"REFUTES\": 2, \"DISPUTED\": 3}\n",
    "\n",
    "# Load evidences\n",
    "with open(evidence_file, 'r') as f:\n",
    "    evidences = json.load(f)\n",
    "print(f\"Loaded {len(evidences)} evidence documents.\")\n",
    "\n",
    "# Load train claims\n",
    "with open(train_claims_file, 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "train_data = []\n",
    "for train_id in train_claims.keys():\n",
    "    claim = train_claims[train_id]['claim_text']\n",
    "    cur_evidences = train_claims[train_id]['evidences']\n",
    "    evidence_texts = [evidences[evidence_id] for evidence_id in cur_evidences]\n",
    "    train_input = cls_token + claim + sep_token.join(evidence_texts) + sep_token\n",
    "    train_data.append({\n",
    "        'input': train_input,\n",
    "        'label': labels2id[train_claims[train_id]['claim_label']],\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(train_data)} train items.\")\n",
    "\n",
    "# Load dev claims\n",
    "with open(dev_claims_file, 'r') as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "dev_data = []\n",
    "for dev_id in dev_claims.keys():\n",
    "    claim = dev_claims[dev_id]['claim_text']\n",
    "    cur_evidences = dev_claims[dev_id]['evidences']\n",
    "    evidence_texts = [evidences[evidence_id] for evidence_id in cur_evidences]\n",
    "    dev_input = cls_token + claim + sep_token.join(evidence_texts) + sep_token\n",
    "    dev_data.append({\n",
    "        'input': dev_input,\n",
    "        'label': labels2id[dev_claims[dev_id]['claim_label']],\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(dev_data)} dev items.\")\n",
    "\n",
    "# Load test claims\n",
    "with open(test_claims_file, 'r') as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "test_data = []\n",
    "test_ids = list(test_claims.keys())\n",
    "for test_id in test_ids:\n",
    "    claim = test_claims[test_id]['claim_text']\n",
    "    cur_evidences = test_claims[test_id]['evidences']\n",
    "    evidence_texts = [evidences[evidence_id] for evidence_id in cur_evidences]\n",
    "    test_input = cls_token + claim + sep_token.join(evidence_texts) + sep_token\n",
    "    test_data.append({\n",
    "        'input': test_input,\n",
    "        'label': -1,  # Placeholder for test data labels\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "seed = 330\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:54:50.593854: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-14 13:54:50.612511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747202090.636633    5517 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747202090.644381    5517 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747202090.667314    5517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747202090.667335    5517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747202090.667339    5517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747202090.667343    5517 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 13:54:50.677709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for train, and test data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Better processing method: using model's standard format\n",
    "        if 'claim_text' in item and 'evidence_texts' in item:\n",
    "            # Assuming each data item contains separate claim and evidences\n",
    "            claim = item['claim_text']\n",
    "            evidences = item['evidence_texts']\n",
    "            evidence_text = \" \".join(evidences[:3])  # Limit number of evidences to avoid truncation\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                claim,                 # Claim as the first text\n",
    "                evidence_text,         # Evidence as the second text\n",
    "                padding='max_length',\n",
    "                truncation='longest_first',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        else:\n",
    "            # Compatible with existing input format\n",
    "            inputs = self.tokenizer(\n",
    "                item['input'], \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "        label = torch.tensor(item['label'])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = ClaimsDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "dev_dataset = ClaimsDataset(dev_data, tokenizer)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=dev_dataset.collate_fn)\n",
    "\n",
    "test_dataset = ClaimsDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Smoothing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Label Smoothing Loss class\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # Create smoothed label distribution\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def validate_model_with_exposure_mitigation(model, exposure_mitigator, dataloader, device, criterion=None):\n",
    "    model.eval()\n",
    "    exposure_mitigator.training = False\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=f\"[Validating]\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass using exposure mitigator\n",
    "            logits = exposure_mitigator.forward_with_scheduled_sampling(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Use label smoothing loss or default cross entropy\n",
    "            if criterion is not None:\n",
    "                loss = criterion(logits, labels)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    model.train()  # Set model back to training mode\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exposure Bias Mitigation for Scheduled Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of scheduled sampling to mitigate exposure bias\n",
    "class ExposureBiasMitigator:\n",
    "    def __init__(self, model, device, num_labels, decay_rate=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The classification model\n",
    "            device: The device to run on\n",
    "            num_labels: Number of classification labels\n",
    "            decay_rate: Rate at which to decrease teacher forcing probability\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.decay_rate = decay_rate\n",
    "        self.teacher_forcing_prob = 1.0\n",
    "        \n",
    "    def update_teacher_forcing_prob(self, epoch):\n",
    "        \"\"\"Decrease teacher forcing probability as training progresses\"\"\"\n",
    "        self.teacher_forcing_prob = max(0.0, 1.0 - self.decay_rate * epoch)\n",
    "        return self.teacher_forcing_prob\n",
    "        \n",
    "    def forward_with_scheduled_sampling(self, input_ids, attention_mask, labels):\n",
    "        \"\"\"Forward pass with scheduled sampling to mitigate exposure bias\"\"\"\n",
    "        # Initial forward pass with all real inputs\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply scheduled sampling for a second forward pass\n",
    "        if self.training and random.random() > self.teacher_forcing_prob:\n",
    "            # Generate pseudo-labels based on model prediction\n",
    "            with torch.no_grad():\n",
    "                pseudo_probs = F.softmax(logits, dim=-1)\n",
    "                # Sample from the distribution\n",
    "                pseudo_labels = torch.multinomial(pseudo_probs, 1).squeeze(-1)\n",
    "            \n",
    "            # Create token type ids based on pseudo-labels\n",
    "            # This simulates the model being conditioned on its own predictions\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            for i, label in enumerate(pseudo_labels):\n",
    "                # Set token type based on predicted class\n",
    "                token_type_ids[i, :] = label.item()\n",
    "                \n",
    "            # Second forward pass with token type ids from predicted labels\n",
    "            if hasattr(self.model, 'token_type_embeddings'):\n",
    "                # If model supports token type embeddings\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Ensemble run different random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient implementation of model ensemble techniques\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EnsembleClassifier:\n",
    "    \"\"\"Memory-efficient implementation of homogeneous ensemble for fact verification\"\"\"\n",
    "    def __init__(self, model_names, device, num_labels=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_names: List of model names to use in ensemble\n",
    "            device: Device to run models on\n",
    "            num_labels: Number of classification labels\n",
    "        \"\"\"\n",
    "        self.model_names = model_names\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.tokenizers = {}\n",
    "        self.models = {}\n",
    "        \n",
    "    def _load_model(self, model_name):\n",
    "        \"\"\"Dynamically load and cache models to save memory\"\"\"\n",
    "        if model_name in self.models:\n",
    "            return self.models[model_name]\n",
    "            \n",
    "        # Clean memory before loading a new model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=self.num_labels\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cache for later use\n",
    "        self.models[model_name] = model\n",
    "        return model\n",
    "        \n",
    "    def _unload_models(self):\n",
    "        \"\"\"Unload all models to free memory\"\"\"\n",
    "        for key in list(self.models.keys()):\n",
    "            del self.models[key]\n",
    "        self.models = {}\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def homogeneous_ensemble(self, dataloader, model_name, model_checkpoint, num_runs=3):\n",
    "        \"\"\"Homogeneous ensemble - Train model multiple times with different seeds\"\"\"\n",
    "        all_predictions = []\n",
    "        all_logits = []\n",
    "        \n",
    "        # Use same model but with different random initializations\n",
    "        base_model = self._load_model(model_name)\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            print(f\"Run {run+1}/{num_runs}\")\n",
    "            \n",
    "            # Set different seed for each run\n",
    "            torch.manual_seed(330 + run)\n",
    "            torch.cuda.manual_seed_all(330 + run)\n",
    "            \n",
    "            # Load the model checkpoint but with different initialization for some layers\n",
    "            model = self._load_model(model_name)\n",
    "            \n",
    "            # Optionally load some pre-trained weights\n",
    "            if model_checkpoint:\n",
    "                model.load_state_dict(torch.load(model_checkpoint), strict=False)\n",
    "                \n",
    "            # Get predictions for this run\n",
    "            model.eval()\n",
    "            run_logits = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(dataloader, desc=f\"Evaluating run {run+1}\"):\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    \n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    batch_logits = outputs.logits\n",
    "                    run_logits.append(batch_logits)\n",
    "            \n",
    "            # Concat all batch logits\n",
    "            run_logits = torch.cat(run_logits, dim=0).cpu()\n",
    "            all_logits.append(run_logits)\n",
    "            \n",
    "            # Free memory\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Combine predictions from all runs (average logits)\n",
    "        avg_logits = torch.stack(all_logits).mean(dim=0)\n",
    "        _, predictions = torch.max(avg_logits, dim=1)\n",
    "        \n",
    "        return predictions.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QIEqDDT78q39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/20 - Teacher forcing probability: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21610f376a74ca5960a3a78fcbbb7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20: Loss: 1.3881, LR: 0.0000020\n",
      "Epoch 2/20 - Teacher forcing probability: 0.9500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f02bd8043c45059d7742f2d33eb0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40: Loss: 1.3142, LR: 0.0000040\n",
      "Step 60: Loss: 1.2798, LR: 0.0000060\n",
      "Epoch 3/20 - Teacher forcing probability: 0.9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48e4a015c7a4871ab5a9f0bf22cb090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 80: Loss: 1.2627, LR: 0.0000080\n",
      "Step 100: Loss: 1.1329, LR: 0.0000100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6b0a4cc9e247c490d8531c4624ed81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2323, Validation Accuracy: 0.5325\n",
      "New best model found! Validation Accuracy: 0.5325\n",
      "Epoch 4/20 - Teacher forcing probability: 0.8500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad1c1fc2a5043e7bc6ea78684afc950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 120: Loss: 1.0500, LR: 0.0000097\n",
      "Step 140: Loss: 1.0097, LR: 0.0000094\n",
      "Epoch 5/20 - Teacher forcing probability: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7141dbabb2a94bae8315d21c7df21731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160: Loss: 0.9519, LR: 0.0000091\n",
      "Step 180: Loss: 0.9200, LR: 0.0000088\n",
      "Epoch 6/20 - Teacher forcing probability: 0.7500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e758b9e996b498f872059bdfde590cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Loss: 0.9874, LR: 0.0000085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11e7b8a4cc54c2182a50c87463315fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0970, Validation Accuracy: 0.5974\n",
      "New best model found! Validation Accuracy: 0.5974\n",
      "Step 220: Loss: 0.8915, LR: 0.0000082\n",
      "Epoch 7/20 - Teacher forcing probability: 0.7000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d383464bd1464f9f8a6cd9dc3f402b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 240: Loss: 0.8124, LR: 0.0000079\n",
      "Step 260: Loss: 0.8384, LR: 0.0000076\n",
      "Epoch 8/20 - Teacher forcing probability: 0.6500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d429d37c779e458d825810b87142e676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 280: Loss: 0.8420, LR: 0.0000074\n",
      "Step 300: Loss: 0.6802, LR: 0.0000071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855b3a07c6fc486983c1c48d637b6b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9980, Validation Accuracy: 0.6753\n",
      "New best model found! Validation Accuracy: 0.6753\n",
      "Epoch 9/20 - Teacher forcing probability: 0.6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd287c70c8e44738687f6b42563365a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 320: Loss: 0.7584, LR: 0.0000068\n",
      "Step 340: Loss: 0.8331, LR: 0.0000065\n",
      "Epoch 10/20 - Teacher forcing probability: 0.5500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e42802bac6a449889730946f44eaf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 360: Loss: 0.6939, LR: 0.0000062\n",
      "Step 380: Loss: 0.8304, LR: 0.0000059\n",
      "Epoch 11/20 - Teacher forcing probability: 0.5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2714b43fdc0641eab9cf2aec6c1bf5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: Loss: 0.8439, LR: 0.0000056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65293a2fd9f41eda9b863cb9a88d17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9794, Validation Accuracy: 0.7078\n",
      "New best model found! Validation Accuracy: 0.7078\n",
      "Step 420: Loss: 0.6995, LR: 0.0000053\n",
      "Epoch 12/20 - Teacher forcing probability: 0.4500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d73ab045caa47cd894efc651bf277b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 440: Loss: 0.7142, LR: 0.0000050\n",
      "Step 460: Loss: 0.6516, LR: 0.0000047\n",
      "Epoch 13/20 - Teacher forcing probability: 0.4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c276add5ad74e95963a814c460b9d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 480: Loss: 0.8082, LR: 0.0000044\n",
      "Step 500: Loss: 0.7932, LR: 0.0000041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b00276055e9461e960858564e906bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9803, Validation Accuracy: 0.6883\n",
      "Epoch 14/20 - Teacher forcing probability: 0.3500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a481a03a47143208f09b86f295aeb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520: Loss: 0.6608, LR: 0.0000038\n",
      "Step 540: Loss: 0.7995, LR: 0.0000035\n",
      "Epoch 15/20 - Teacher forcing probability: 0.3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e410c9528e45482595af70d37f6908bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 560: Loss: 0.6486, LR: 0.0000032\n",
      "Step 580: Loss: 0.6713, LR: 0.0000029\n",
      "Epoch 16/20 - Teacher forcing probability: 0.2500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98edc4e8024b404fa257e96cd1bb13c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600: Loss: 0.9028, LR: 0.0000026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b0c304d34648089f5923f3b9f01877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0241, Validation Accuracy: 0.6558\n",
      "Step 620: Loss: 0.6037, LR: 0.0000024\n",
      "Epoch 17/20 - Teacher forcing probability: 0.2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44d28a9e302464fb82647261ff22d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 640: Loss: 0.7332, LR: 0.0000021\n",
      "Step 660: Loss: 0.7272, LR: 0.0000018\n",
      "Epoch 18/20 - Teacher forcing probability: 0.1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30567eeb97d4048a35e4568a90e62ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 680: Loss: 0.5898, LR: 0.0000015\n",
      "Step 700: Loss: 0.5980, LR: 0.0000012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b52ce50c3349b9b7450f06f234dbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0251, Validation Accuracy: 0.6688\n",
      "Epoch 19/20 - Teacher forcing probability: 0.1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1599c142c1594f77a47d24e25cf4cdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 720: Loss: 0.7020, LR: 0.0000009\n",
      "Step 740: Loss: 0.5377, LR: 0.0000006\n",
      "Epoch 20/20 - Teacher forcing probability: 0.0500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a030138f252d4bee885d216c65042cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 760: Loss: 0.7885, LR: 0.0000003\n",
      "Step 780: Loss: 0.5904, LR: 0.0000000\n",
      "Training complete! Best validation accuracy: 0.7078\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize exposure bias mitigator\n",
    "exposure_mitigator = ExposureBiasMitigator(model, device, len(id2labels), decay_rate=0.05)\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "learning_rate = 1e-5\n",
    "# Define number of epochs\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Add learning rate scheduler to improve training stability\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,  # Warmup steps\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Validation interval\n",
    "val_interval = 100\n",
    "\n",
    "# Logging interval\n",
    "log_interval = 20\n",
    "\n",
    "# Define label smoothing loss function\n",
    "criterion = LabelSmoothingLoss(classes=len(id2labels), smoothing=0.1)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "# Training phase\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # if step > 120:\n",
    "    #         break\n",
    "    # Update teacher forcing probability\n",
    "    tf_prob = exposure_mitigator.update_teacher_forcing_prob(epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Teacher forcing probability: {tf_prob:.4f}\")\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
    "    for batch in progress_bar:\n",
    "        # if step > 120:\n",
    "        #     break\n",
    "\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # logits = outputs.logits\n",
    "        exposure_mitigator.training = True\n",
    "        logits = exposure_mitigator.forward_with_scheduled_sampling(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # Use label smoothing loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "         # Gradient clipping to prevent gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n",
    "        step += 1\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print(f\"Step {step}: Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.7f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        if step % val_interval == 0:\n",
    "            # Set exposure mitigator to eval mode\n",
    "            exposure_mitigator.training = False\n",
    "            val_loss, val_accuracy = validate_model_with_exposure_mitigation(\n",
    "                model, exposure_mitigator, dev_dataloader, device, criterion\n",
    "            )\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Save best model based on validation F1\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                print(f\"New best model found! Validation Accuracy: {val_accuracy:.4f}\")\n",
    "                # Save the model state\n",
    "                torch.save(model.state_dict(), \"best_classification_model.pth\")\n",
    "                # Early stopping if accuracy reaches high threshold\n",
    "                if val_accuracy > 0.75:\n",
    "                    print(f\"Reached high accuracy ({val_accuracy:.4f}). Early stopping.\")\n",
    "                    break\n",
    "\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzGuzHPE87Ya"
   },
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6ZVeNYIH9IaL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Homogeneous Ensemble...\n",
      "Loading model: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating run 1: 100%|██████████| 5/5 [00:00<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating run 2: 100%|██████████| 5/5 [00:00<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating run 3: 100%|██████████| 5/5 [00:00<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Distribution:\n",
      "Counter({'NOT_ENOUGH_INFO': 112, 'SUPPORTS': 37, 'DISPUTED': 4})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to classification_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define model to use\n",
    "model_name = \"distilbert-base-uncased\"  # Use the model you've trained with\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create ensemble classifier\n",
    "ensemble = EnsembleClassifier([model_name], device, num_labels=len(id2labels))\n",
    "\n",
    "# Run homogeneous ensemble\n",
    "print(\"Running Homogeneous Ensemble...\")\n",
    "predictions = ensemble.homogeneous_ensemble(\n",
    "    test_dataloader,\n",
    "    model_name=model_name,\n",
    "    model_checkpoint=\"best_classification_model.pth\",\n",
    "    num_runs=3\n",
    ")\n",
    "predicted_labels = [id2labels[pred] for pred in predictions]\n",
    "\n",
    "# Visualize results\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # If ground truth is available\n",
    "    if 'ground_truth' in locals() and ground_truth:\n",
    "        # Convert labels to IDs\n",
    "        label2id = {v: k for k, v in id2labels.items()}\n",
    "        gt_ids = [label2id[label] for label in ground_truth]\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(gt_ids, predictions)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=list(id2labels.values()),\n",
    "                   yticklabels=list(id2labels.values()))\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.title('Homogeneous Ensemble Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('homogeneous_confusion_matrix.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(gt_ids, predictions, target_names=list(id2labels.values())))\n",
    "    else:\n",
    "        # Just show prediction distribution\n",
    "        from collections import Counter\n",
    "        print(\"\\nPrediction Distribution:\")\n",
    "        print(Counter(predicted_labels))\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        counter = Counter(predicted_labels)\n",
    "        plt.bar(counter.keys(), counter.values())\n",
    "        plt.title('Prediction Distribution')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig('prediction_distribution.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Could not generate visualization (matplotlib or seaborn not available)\")\n",
    "\n",
    "# Create results dictionary\n",
    "results = {}\n",
    "for i, test_id in enumerate(test_ids):\n",
    "    results[test_id] = {\n",
    "        'claim_text': test_claims[test_id]['claim_text'],\n",
    "        'claim_label': predicted_labels[i],\n",
    "        'evidences': test_claims[test_id]['evidences']\n",
    "    }\n",
    "\n",
    "# Save results to file\n",
    "with open('classification_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to classification_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mefSOe8eTmGP"
   },
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
