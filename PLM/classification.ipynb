{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2025 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qvff21Hv8zjk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1208827 evidence documents.\n",
      "Loaded 1228 train items.\n",
      "Loaded 154 dev items.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "data_dir = \"data\"\n",
    "train_claims_file = os.path.join(data_dir, \"train-claims.json\")\n",
    "dev_claims_file = os.path.join(data_dir, \"dev-claims.json\")\n",
    "test_claims_file = os.path.join(data_dir, \"test_claims_retrieved_reranked.json\")\n",
    "evidence_file = os.path.join(data_dir, \"evidence.json\")\n",
    "\n",
    "sep_token = \"[SEP]\"\n",
    "cls_token = \"[CLS]\"\n",
    "\n",
    "id2labels = [\"SUPPORTS\", \"NOT_ENOUGH_INFO\", \"REFUTES\", \"DISPUTED\"]\n",
    "labels2id = {\"SUPPORTS\": 0, \"NOT_ENOUGH_INFO\": 1, \"REFUTES\": 2, \"DISPUTED\": 3}\n",
    "\n",
    "# Load evidences\n",
    "with open(evidence_file, 'r') as f:\n",
    "    evidences = json.load(f)\n",
    "print(f\"Loaded {len(evidences)} evidence documents.\")\n",
    "\n",
    "# Load train claims\n",
    "with open(train_claims_file, 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "train_data = []\n",
    "for train_id in train_claims.keys():\n",
    "    claim = train_claims[train_id]['claim_text']\n",
    "    cur_evidences = train_claims[train_id]['evidences']\n",
    "    evidence_texts = [evidences[evidence_id] for evidence_id in cur_evidences]\n",
    "    train_input = cls_token + claim + sep_token.join(evidence_texts) + sep_token\n",
    "    train_data.append({\n",
    "        'input': train_input,\n",
    "        'label': labels2id[train_claims[train_id]['claim_label']],\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(train_data)} train items.\")\n",
    "\n",
    "# Load dev claims\n",
    "with open(dev_claims_file, 'r') as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "dev_data = []\n",
    "for dev_id in dev_claims.keys():\n",
    "    claim = dev_claims[dev_id]['claim_text']\n",
    "    cur_evidences = dev_claims[dev_id]['evidences']\n",
    "    evidence_texts = [evidences[evidence_id] for evidence_id in cur_evidences]\n",
    "    dev_input = cls_token + claim + sep_token.join(evidence_texts) + sep_token\n",
    "    dev_data.append({\n",
    "        'input': dev_input,\n",
    "        'label': labels2id[dev_claims[dev_id]['claim_label']],\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(dev_data)} dev items.\")\n",
    "\n",
    "# Load test claims\n",
    "with open(test_claims_file, 'r') as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "test_data = []\n",
    "test_ids = list(test_claims.keys())\n",
    "for test_id in test_ids:\n",
    "    claim = test_claims[test_id]['claim_text']\n",
    "    cur_evidences = test_claims[test_id]['evidences']\n",
    "    evidence_texts = [evidences[evidence_id] for evidence_id in cur_evidences]\n",
    "    test_input = cls_token + claim + sep_token.join(evidence_texts) + sep_token\n",
    "    test_data.append({\n",
    "        'input': test_input,\n",
    "        'label': -1,  # Placeholder for test data labels\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "seed = 330\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 17:59:42.780586: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 17:59:42.801127: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747303182.827032    3012 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747303182.834899    3012 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747303182.854326    3012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747303182.854345    3012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747303182.854348    3012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747303182.854350    3012 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 17:59:42.861092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for train, and test data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Better processing method: using model's standard format\n",
    "        if 'claim_text' in item and 'evidence_texts' in item:\n",
    "            # Assuming each data item contains separate claim and evidences\n",
    "            claim = item['claim_text']\n",
    "            evidences = item['evidence_texts']\n",
    "            evidence_text = \" \".join(evidences[:3])  # Limit number of evidences to avoid truncation\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                claim,                 # Claim as the first text\n",
    "                evidence_text,         # Evidence as the second text\n",
    "                padding='max_length',\n",
    "                truncation='longest_first',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        else:\n",
    "            # Compatible with existing input format\n",
    "            inputs = self.tokenizer(\n",
    "                item['input'], \n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "        label = torch.tensor(item['label'])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = ClaimsDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "dev_dataset = ClaimsDataset(dev_data, tokenizer)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=dev_dataset.collate_fn)\n",
    "\n",
    "test_dataset = ClaimsDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Smoothing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Label Smoothing Loss class\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # Create smoothed label distribution\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def validate_model_with_exposure_mitigation(model, exposure_mitigator, dataloader, device, criterion=None):\n",
    "    model.eval()\n",
    "    exposure_mitigator.training = False\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=f\"[Validating]\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass using exposure mitigator\n",
    "            logits = exposure_mitigator.forward_with_scheduled_sampling(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Use label smoothing loss or default cross entropy\n",
    "            if criterion is not None:\n",
    "                loss = criterion(logits, labels)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    model.train()  # Set model back to training mode\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exposure Bias Mitigation for Scheduled Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of scheduled sampling to mitigate exposure bias\n",
    "class ExposureBiasMitigator:\n",
    "    def __init__(self, model, device, num_labels, decay_rate=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The classification model\n",
    "            device: The device to run on\n",
    "            num_labels: Number of classification labels\n",
    "            decay_rate: Rate at which to decrease teacher forcing probability\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.decay_rate = decay_rate\n",
    "        self.teacher_forcing_prob = 1.0\n",
    "        \n",
    "    def update_teacher_forcing_prob(self, epoch):\n",
    "        \"\"\"Decrease teacher forcing probability as training progresses\"\"\"\n",
    "        self.teacher_forcing_prob = max(0.0, 1.0 - self.decay_rate * epoch)\n",
    "        return self.teacher_forcing_prob\n",
    "        \n",
    "    def forward_with_scheduled_sampling(self, input_ids, attention_mask, labels):\n",
    "        \"\"\"Forward pass with scheduled sampling to mitigate exposure bias\"\"\"\n",
    "        # Initial forward pass with all real inputs\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Apply scheduled sampling for a second forward pass\n",
    "        if self.training and random.random() > self.teacher_forcing_prob:\n",
    "            # Generate pseudo-labels based on model prediction\n",
    "            with torch.no_grad():\n",
    "                pseudo_probs = F.softmax(logits, dim=-1)\n",
    "                # Sample from the distribution\n",
    "                pseudo_labels = torch.multinomial(pseudo_probs, 1).squeeze(-1)\n",
    "            \n",
    "            # Create token type ids based on pseudo-labels\n",
    "            # This simulates the model being conditioned on its own predictions\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            for i, label in enumerate(pseudo_labels):\n",
    "                # Set token type based on predicted class\n",
    "                token_type_ids[i, :] = label.item()\n",
    "                \n",
    "            # Second forward pass with token type ids from predicted labels\n",
    "            if hasattr(self.model, 'token_type_embeddings'):\n",
    "                # If model supports token type embeddings\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Ensemble run different random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient implementation of model ensemble techniques\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EnsembleClassifier:\n",
    "    \"\"\"Memory-efficient implementation of homogeneous ensemble for fact verification\"\"\"\n",
    "    def __init__(self, model_names, device, num_labels=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_names: List of model names to use in ensemble\n",
    "            device: Device to run models on\n",
    "            num_labels: Number of classification labels\n",
    "        \"\"\"\n",
    "        self.model_names = model_names\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.tokenizers = {}\n",
    "        self.models = {}\n",
    "        \n",
    "    def _load_model(self, model_name):\n",
    "        \"\"\"Dynamically load and cache models to save memory\"\"\"\n",
    "        if model_name in self.models:\n",
    "            return self.models[model_name]\n",
    "            \n",
    "        # Clean memory before loading a new model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=self.num_labels\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Cache for later use\n",
    "        self.models[model_name] = model\n",
    "        return model\n",
    "        \n",
    "    def _unload_models(self):\n",
    "        \"\"\"Unload all models to free memory\"\"\"\n",
    "        for key in list(self.models.keys()):\n",
    "            del self.models[key]\n",
    "        self.models = {}\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def homogeneous_ensemble(self, dataloader, model_name, model_checkpoint, num_runs=3):\n",
    "        \"\"\"Homogeneous ensemble - Train model multiple times with different seeds\"\"\"\n",
    "        all_predictions = []\n",
    "        all_logits = []\n",
    "        \n",
    "        # Use same model but with different random initializations\n",
    "        base_model = self._load_model(model_name)\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            print(f\"Run {run+1}/{num_runs}\")\n",
    "            \n",
    "            # Set different seed for each run\n",
    "            torch.manual_seed(330 + run)\n",
    "            torch.cuda.manual_seed_all(330 + run)\n",
    "            \n",
    "            # Load the model checkpoint but with different initialization for some layers\n",
    "            model = self._load_model(model_name)\n",
    "            \n",
    "            # Optionally load some pre-trained weights\n",
    "            if model_checkpoint:\n",
    "                model.load_state_dict(torch.load(model_checkpoint), strict=False)\n",
    "                \n",
    "            # Get predictions for this run\n",
    "            model.eval()\n",
    "            run_logits = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(dataloader, desc=f\"Evaluating run {run+1}\"):\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    \n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    batch_logits = outputs.logits\n",
    "                    run_logits.append(batch_logits)\n",
    "            \n",
    "            # Concat all batch logits\n",
    "            run_logits = torch.cat(run_logits, dim=0).cpu()\n",
    "            all_logits.append(run_logits)\n",
    "            \n",
    "            # Free memory\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Combine predictions from all runs (average logits)\n",
    "        avg_logits = torch.stack(all_logits).mean(dim=0)\n",
    "        _, predictions = torch.max(avg_logits, dim=1)\n",
    "        \n",
    "        return predictions.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QIEqDDT78q39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/20 - Teacher forcing probability: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59c8ea76b4e4a6dba46670c2c2654ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20: Loss: 1.3881, LR: 0.0000020\n",
      "Epoch 2/20 - Teacher forcing probability: 0.9500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec833543ee86482ea22e01888b647c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40: Loss: 1.3142, LR: 0.0000040\n",
      "Step 60: Loss: 1.2798, LR: 0.0000060\n",
      "Epoch 3/20 - Teacher forcing probability: 0.9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d425cda6129d44f9a020da307d1f9cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 80: Loss: 1.2627, LR: 0.0000080\n",
      "Step 100: Loss: 1.1329, LR: 0.0000100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aebab718b1a457bbee5fbe41f03f0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2323, Validation Accuracy: 0.5325\n",
      "New best model found! Validation Accuracy: 0.5325\n",
      "Epoch 4/20 - Teacher forcing probability: 0.8500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764e1b2b51864dbf91eca25fc3369f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 120: Loss: 1.0500, LR: 0.0000097\n",
      "Step 140: Loss: 1.0097, LR: 0.0000094\n",
      "Epoch 5/20 - Teacher forcing probability: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714dd005957347d284ca5c273de63f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160: Loss: 0.9519, LR: 0.0000091\n",
      "Step 180: Loss: 0.9200, LR: 0.0000088\n",
      "Epoch 6/20 - Teacher forcing probability: 0.7500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760b2e26f18742a8aa966a8a06672276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Loss: 0.9874, LR: 0.0000085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb46cd51fcbe452d881dd02d88a62a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0970, Validation Accuracy: 0.5974\n",
      "New best model found! Validation Accuracy: 0.5974\n",
      "Step 220: Loss: 0.8915, LR: 0.0000082\n",
      "Epoch 7/20 - Teacher forcing probability: 0.7000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862dec2088ad4d5da54f6106ea69abc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 240: Loss: 0.8124, LR: 0.0000079\n",
      "Step 260: Loss: 0.8384, LR: 0.0000076\n",
      "Epoch 8/20 - Teacher forcing probability: 0.6500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d099ae999e04a7c941f7a457548bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 280: Loss: 0.8420, LR: 0.0000074\n",
      "Step 300: Loss: 0.6802, LR: 0.0000071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6c2fdfcca344ac840dbf8572aa3844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9980, Validation Accuracy: 0.6753\n",
      "New best model found! Validation Accuracy: 0.6753\n",
      "Epoch 9/20 - Teacher forcing probability: 0.6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4b645a0de54c428f75924bd3445fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 320: Loss: 0.7584, LR: 0.0000068\n",
      "Step 340: Loss: 0.8330, LR: 0.0000065\n",
      "Epoch 10/20 - Teacher forcing probability: 0.5500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7d4599af2e4b569d9ada24a7bf409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 360: Loss: 0.6940, LR: 0.0000062\n",
      "Step 380: Loss: 0.8303, LR: 0.0000059\n",
      "Epoch 11/20 - Teacher forcing probability: 0.5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aa36e407d044a989f4398e127db16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: Loss: 0.8440, LR: 0.0000056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0004bc8449d4a41a750fac24b2603f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9794, Validation Accuracy: 0.7078\n",
      "New best model found! Validation Accuracy: 0.7078\n",
      "Step 420: Loss: 0.7000, LR: 0.0000053\n",
      "Epoch 12/20 - Teacher forcing probability: 0.4500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0191e08513467ab13326143c95cad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 440: Loss: 0.7139, LR: 0.0000050\n",
      "Step 460: Loss: 0.6515, LR: 0.0000047\n",
      "Epoch 13/20 - Teacher forcing probability: 0.4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa0e25683c74b359fd74c9b5d535d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 480: Loss: 0.8083, LR: 0.0000044\n",
      "Step 500: Loss: 0.7935, LR: 0.0000041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb005d4f5a0c41d29826728caac0b58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9803, Validation Accuracy: 0.6883\n",
      "Epoch 14/20 - Teacher forcing probability: 0.3500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f65376b8ca24b62bae9667b973b4400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520: Loss: 0.6609, LR: 0.0000038\n",
      "Step 540: Loss: 0.7993, LR: 0.0000035\n",
      "Epoch 15/20 - Teacher forcing probability: 0.3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f6a3c7251440fc841ccf7dd3d5db27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 560: Loss: 0.6488, LR: 0.0000032\n",
      "Step 580: Loss: 0.6714, LR: 0.0000029\n",
      "Epoch 16/20 - Teacher forcing probability: 0.2500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4728e4723fe5435c80b6c68151a385c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600: Loss: 0.9031, LR: 0.0000026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d247272cc142c394e2472e0151231a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0236, Validation Accuracy: 0.6558\n",
      "Step 620: Loss: 0.6037, LR: 0.0000024\n",
      "Epoch 17/20 - Teacher forcing probability: 0.2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6809e668f0464fb38c7a4f0a73590586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 640: Loss: 0.7332, LR: 0.0000021\n",
      "Step 660: Loss: 0.7274, LR: 0.0000018\n",
      "Epoch 18/20 - Teacher forcing probability: 0.1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6397cab52c414c32829a1486885a902c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 680: Loss: 0.5896, LR: 0.0000015\n",
      "Step 700: Loss: 0.5980, LR: 0.0000012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aba931e7abf4f7c8b0a041707e6751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Validating]:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0249, Validation Accuracy: 0.6753\n",
      "Epoch 19/20 - Teacher forcing probability: 0.1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4301416844c4445d8b06604db88d35b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 720: Loss: 0.7021, LR: 0.0000009\n",
      "Step 740: Loss: 0.5379, LR: 0.0000006\n",
      "Epoch 20/20 - Teacher forcing probability: 0.0500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493c7babef544d458e2562f3f4ae664a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 [Training]:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 760: Loss: 0.7883, LR: 0.0000003\n",
      "Step 780: Loss: 0.5907, LR: 0.0000000\n",
      "Training complete! Best validation accuracy: 0.7078\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize exposure bias mitigator\n",
    "exposure_mitigator = ExposureBiasMitigator(model, device, len(id2labels), decay_rate=0.05)\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "learning_rate = 1e-5\n",
    "# Define number of epochs\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Add learning rate scheduler to improve training stability\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,  # Warmup steps\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Validation interval\n",
    "val_interval = 100\n",
    "\n",
    "# Logging interval\n",
    "log_interval = 20\n",
    "\n",
    "# Define label smoothing loss function\n",
    "criterion = LabelSmoothingLoss(classes=len(id2labels), smoothing=0.1)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "# Training phase\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # if step > 120:\n",
    "    #         break\n",
    "    # Update teacher forcing probability\n",
    "    tf_prob = exposure_mitigator.update_teacher_forcing_prob(epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Teacher forcing probability: {tf_prob:.4f}\")\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
    "    for batch in progress_bar:\n",
    "        # if step > 120:\n",
    "        #     break\n",
    "\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # logits = outputs.logits\n",
    "        exposure_mitigator.training = True\n",
    "        logits = exposure_mitigator.forward_with_scheduled_sampling(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # Use label smoothing loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "         # Gradient clipping to prevent gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n",
    "        step += 1\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print(f\"Step {step}: Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.7f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        if step % val_interval == 0:\n",
    "            # Set exposure mitigator to eval mode\n",
    "            exposure_mitigator.training = False\n",
    "            val_loss, val_accuracy = validate_model_with_exposure_mitigation(\n",
    "                model, exposure_mitigator, dev_dataloader, device, criterion\n",
    "            )\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Save best model based on validation F1\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                print(f\"New best model found! Validation Accuracy: {val_accuracy:.4f}\")\n",
    "                # Save the model state\n",
    "                torch.save(model.state_dict(), \"best_classification_model.pth\")\n",
    "                # Early stopping if accuracy reaches high threshold\n",
    "                if val_accuracy > 0.75:\n",
    "                    print(f\"Reached high accuracy ({val_accuracy:.4f}). Early stopping.\")\n",
    "                    break\n",
    "\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzGuzHPE87Ya"
   },
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6ZVeNYIH9IaL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Homogeneous Ensemble...\n",
      "Loading model: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating run 1: 100%|██████████| 5/5 [00:00<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating run 2: 100%|██████████| 5/5 [00:00<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating run 3: 100%|██████████| 5/5 [00:00<00:00, 14.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Distribution:\n",
      "Counter({'NOT_ENOUGH_INFO': 111, 'SUPPORTS': 34, 'DISPUTED': 7, 'REFUTES': 1})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to classification_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define model to use\n",
    "model_name = \"distilbert-base-uncased\"  # Use the model you've trained with\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create ensemble classifier\n",
    "ensemble = EnsembleClassifier([model_name], device, num_labels=len(id2labels))\n",
    "\n",
    "# Run homogeneous ensemble\n",
    "print(\"Running Homogeneous Ensemble...\")\n",
    "predictions = ensemble.homogeneous_ensemble(\n",
    "    test_dataloader,\n",
    "    model_name=model_name,\n",
    "    model_checkpoint=\"best_classification_model.pth\",\n",
    "    num_runs=3\n",
    ")\n",
    "predicted_labels = [id2labels[pred] for pred in predictions]\n",
    "\n",
    "# Visualize results\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # If ground truth is available\n",
    "    if 'ground_truth' in locals() and ground_truth:\n",
    "        # Convert labels to IDs\n",
    "        label2id = {v: k for k, v in id2labels.items()}\n",
    "        gt_ids = [label2id[label] for label in ground_truth]\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(gt_ids, predictions)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=list(id2labels.values()),\n",
    "                   yticklabels=list(id2labels.values()))\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.title('Homogeneous Ensemble Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('homogeneous_confusion_matrix.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(gt_ids, predictions, target_names=list(id2labels.values())))\n",
    "    else:\n",
    "        # Just show prediction distribution\n",
    "        from collections import Counter\n",
    "        print(\"\\nPrediction Distribution:\")\n",
    "        print(Counter(predicted_labels))\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        counter = Counter(predicted_labels)\n",
    "        plt.bar(counter.keys(), counter.values())\n",
    "        plt.title('Prediction Distribution')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig('prediction_distribution.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Could not generate visualization (matplotlib or seaborn not available)\")\n",
    "\n",
    "# Create results dictionary\n",
    "results = {}\n",
    "for i, test_id in enumerate(test_ids):\n",
    "    results[test_id] = {\n",
    "        'claim_text': test_claims[test_id]['claim_text'],\n",
    "        'claim_label': predicted_labels[i],\n",
    "        'evidences': test_claims[test_id]['evidences']\n",
    "    }\n",
    "\n",
    "# Save results to file\n",
    "with open('classification_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to classification_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mefSOe8eTmGP"
   },
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
